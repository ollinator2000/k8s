
# Kubernetes vom 12. bis 14. Februar 2020

# Backlog

* k8s-Cluster-Deployment
* Ansible-Container


   apt-get install apt-transport-https ca-certificates curl software-properties-common
   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
   add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
   apt-get install docker-ce-cli


  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
  chmod +x ./kubectl
  mv ./kubectl /usr/local/bin/kubectl


echo "source <(kubectl completion bash)" >> ~/.bashrc

curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.7.2/minikube-linux-amd64
chmod +x minikube
mv minikube /usr/local/bin


minikube start --cpus 4 --memory 8192 --network-plugin=cni
(Beenden = minikube stop)


snap install code --classic

kubectl create -f https://raw.githubusercontent.com/cilium/cilium/HEAD/install/kubernetes/quick-install.yaml


1. kubectl describe pod/httpod untersuchen
2. Mit Docker-Daemon verbinden
    eval $(minikube -p minikube docker-env)
3. Welche Container wurden für den Pod erstellt und welche IP-Adressen haben sie?
4. Webserver-Container des Pods killen (docker rm -f)


kubectl exec -ti httpod -c webserver /bin/bash

kubectl get pods -o wide

apiVersion: v1
kind: Pod
metadata:
  name: httpod
spec:
  containers:
  - name: webserver
    image: httpd:latest
  - name: debug
    image: ubuntu:latest
    stdin: true
    tty: true
    command:
    - /bin/bash
    - -c
    args:
    - while true; do echo foo; sleep 1; done;


apiVersion: v1
kind: Pod
metadata:
  name: podtime
spec:
  containers:
  - name: client
    image: alpine:latest
    command:
    - /bin/sh
    - -c
    args:
    - while true; do wget -O - -q localhost/date.txt; sleep 1; done
  - name: clock
    image: ubuntu:latest
    command:
    - /bin/bash
    - -c
    args:
    - while true; do date > /mnt/date.txt; sleep 1; done
    volumeMounts:
    - mountPath: /mnt
      name: htdocs
  - name: webserver
    image: httpd:latest
    volumeMounts:
    - mountPath: /usr/local/apache2/htdocs
      name: htdocs
  volumes:
  - name: htdocs
    emptyDir:
        
        
        
        kubectl run debug --image=alpine -ti --restart=Never

https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/


Übung zu Labels
kubectl run web1 --restart=Never --image=httpd --labels app=aapp,component=c1,version=1.0
kubectl run web2 --restart=Never --image=httpd --labels app=aapp,component=c1,version=1.0
kubectl run web3 --restart=Never --image=httpd --labels app=aapp,component=c2,version=1.1
kubectl run web4 --restart=Never --image=httpd --labels app=aapp,component=c3,version=1.0
kubectl run web5 --restart=Never --image=httpd --labels app=bapp,component=c1,version=1.1
kubectl run web6 --restart=Never --image=httpd --labels app=bapp,component=c2,version=1.0
kubectl run web7 --restart=Never --image=httpd --labels app=bapp,component=c3,version=1.1
kubectl run web8 --restart=Never --image=httpd --labels app=bapp,component=c4,version=1.1

Folgende Pods raussuchen:

Alle Pods, die eine Komponente in Version 1.1 haben
Alle Pods, die Komponente c1 haben
Alle Pods, die Komponente c1 oder c2 haben
Alle Pods, die eine andere Komponente als c2 in Version 1.1 haben
Mit kubectl label --overwrite alle Pods mit Komponente c2 auf Version 1.2 setzen
Mit kubectl label --overwrite alle Pods mit Komponente c3 oder c4 in Version 1.1 auf Version 1.3 setzen

Lösungen:
kubectl get pods -l version=1.1 --show-labels
kubectl get pods -l component=c1 --show-labels
kubectl get pods -l 'component in (c1, c2)' --show-labels
kubectl get pods -l 'component notin (c2), version=1.1' --show-labels
kubectl label pods --overwrite version=1.2 -l component=c2
kubectl label pods --overwrite version=1.3 -l 'component in (c3, c4), version=1.1'




# Webtime-Musterlösung


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: htdocs
  labels:
    app: k8scoins
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
      
      
      
apiVersion: v1
kind: Pod
metadata:
  name: clock
  labels:
    app: k8scoins
    component: clock
    version: "1.0"
spec:
  containers:
  - name: clock
    image: ubuntu:latest
    command:
    - /bin/bash
    - -c
    args:
    - while true; do date > /mnt/date.txt; sleep 1; done
    volumeMounts:
    - mountPath: /mnt
      name: htdocs
  volumes:
  - name: htdocs
    persistentVolumeClaim:
      claimName: htdocs
      
      
      
apiVersion: v1
kind: Pod
metadata:
  name: server
  labels:
    app: k8scoins
    component: server
    version: "1.0"
spec:
  containers:
    image: httpd:latest
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
    volumeMounts:
    - mountPath: /usr/local/apache2/htdocs
      name: htdocs
  volumes:
  - name: htdocs
    persistentVolumeClaim:
      claimName: htdocs
      
      
      
apiVersion: v1
kind: Service
metadata:
  name: k8stime
  labels:
    app: k8stime
spec:
  type: NodePort
  ports:
  - port: 80
    protocol: TCP
    targetPort: http
  selector:
    app: k8scoins
    component: server
    version: "1.0"
    
    
apiVersion: v1
kind: Pod
metadata:
  name: client
  labels:
    app: k8scoins
    component: client
    version: "1.0"
spec:
  containers:
  - name: client
    image: alpine:latest
    command:
    - /bin/sh
    - -c
    args:
    - while true; do wget -O - -q k8stime/date.txt; sleep 1; done
  volumes:
  - name: htdocs
    persistentVolumeClaim:
      claimName: htdocs


Anzeigen der IP-Adresse des Clusters:
minikube ip


Übung 6:
1. Webserver mit Readyness-Probe
2. Service für Webserver
3. Prüfen, dass der Webserver mit fehlschlagender Probe nicht in den EPs ist
4. Webserver ready machen

5. Lifeness-Probe hinzufügen
6. Rausfinden was passiert, wenn die fehlschlägt


https://container.training/


kubectl create secret generic dbpass --from-literal=password=Geheim

https://medium.com/@gmaliar/dynamic-secrets-on-kubernetes-pods-using-vault-35d9094d169

docker run registry
curl 172.17.0.2:5000/v2/_catalog

1. PVC für Images definieren
2. Pod für Registry definieren (auf Labels achten!)
3. Service für Registry definieren

  381  minikube ip
  382  curl 192.168.39.12:31446/v2/_catalog
  383  minikube docker-env
  384  eval $(minikube -p minikube docker-env)
  385  docker info
  386  docker pull alpine:latest
  387  docker tag alpine:latest 10.102.18.28:5000/alpine:latest
  388  docker image ls | grep alpine
  389  docker push 10.102.18.28:5000/alpine:latest
  390  curl 192.168.39.12:31446/v2/_catalog


	git clone https://github.com/jpetazzo/container.training

eval $(minikube -p minikube docker-env)

https://regexcrossword.com/


minikube addons enable ingress


watch -n 1 kubectl get deploy,rs,pods
kubectl scale deploy/worker --replicas=10
kubectl set image deploy/worker worker=httpd:latest --record
kubectl rollout undo deploy/worker --to-revision=2

https://kubernetespodcast.com

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ds1
spec:
  selector:
    matchLabels:
      app: ds1
  template:
    metadata:
      labels:
        app: ds1
    spec:
      containers:
      - name: webui
        image: 10.102.18.28:5000/webui:latest
      nodeSelector:
        nodetype: linuxhotel


 apk add stress-ng

apiVersion: v1
kind: Pod
metadata:
  name: stress
spec:
 containers:
 - name: stresstest
   image: alpine:latest
   command: ["/bin/sh", "-c", "apk add stress-ng ; stress-ng -c4"]
   stdin: true
   tty: true
   resources:          
    requests:
     memory: "128Mi"
     cpu: "1000m"
    limits:
     memory: "256Mi"
     cpu: "1" 

minikube addons enable metrics-server
kubectl top nodes
kubectl top pods



apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: stress
spec:
  scaleTargetRef:
    apiVersion: app/v1
    kind: Deployment
    name: stress
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 50
  
  
   watch -n 1 kubectl get hpa,deploy,rs,pod
   
   
   
git clone https://github.com/kubernetes/autoscaler.git
cd /vertical-pod-autoscaler
./hack/vpa-up.sh


kubectl create ns ...

LimitRange-Objekt mit Default und max für container in den neuen Namespace

Pod ohne Requests/Limits
Pod mit zu hohen Requests/Limits




apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
      
      
      
      
      
      
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rng
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: rng
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: k8scoins
          component: worker
    ports:
    - port: 80
      protocol: TCP


yammels der Übung 23 (für das Beispiel Dockercoin)
netpol-default.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP

netpol-hasher.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: hasher
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: hasher
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: k8scoins
          component: worker
    ports:
    - port: 80
      protocol: TCP

netpol-redis.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: redis
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchExpressions:
        - key: component
          operator: In
          values:
          - webui
          - worker
    ports:
    - port: 6379
      protocol: TCP

netpol-rng.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rng
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: rng
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: k8scoins
          component: worker
    ports:
    - port: 80
      protocol: TCP

netpol-webui.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: webui
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: webui
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - ports:
    - port: 80
      protocol: TCP
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: k8scoins
          component: redis
    ports:
    - port: 6379
      protocol: TCP

netpol-worker.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: worker
spec:
  podSelector:
    matchLabels:
      app: k8scoins
      component: worker
  policyTypes:
  - Ingress
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: k8scoins
        matchExpressions:
        - key: component
          operator: In
          values:
          - rng
          - hasher
    ports:
    - port: 80
      protocol: TCP
  - to:
    - podSelector:
        matchLabels:
          app: k8scoins
          component: redis
    ports:
    - port: 6379
      protocol: TCP






apt-get install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
apt-get install docker-ce docker-ce-cli


curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF | tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl



https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

